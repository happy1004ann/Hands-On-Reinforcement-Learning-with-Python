{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving Frozen Lake Problem Using Value Iteration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal:\n",
    "\n",
    "Imagine, there is a frozen lake from your home to office, you should walk on the frozen lake\n",
    "to reach your office. But oops! there will be a hole in the frozen lake in between, so you have\n",
    "to be careful while walking in the frozen lake to avoid getting trapped at holes.\n",
    "Look at the below figure where, \n",
    "\n",
    "1. S is the starting position (Home)\n",
    "2. F is the Frozen lake where you can walk\n",
    "3. H is the Hole which you have to be so careful about\n",
    "4. G is the Goal (office)\n",
    "\n",
    "![title](images/B09792_03_50.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Okay, now let us use our agent instead of you to find the correct way to reach the office.\n",
    "The agent goal is to find the optimal path to reach from S to G without getting trapped at H.\n",
    "How an agent can achieve this? We give +1 point as a reward to the agent if it correctly\n",
    "walks on the frozen lake and 0 points if it falls into the hole. So that agent could determine\n",
    "which is the right action. An agent will now try to find the optimal policy. Optimal policy\n",
    "implies taking the correct path which maximizes the agent reward. If the agent is\n",
    "maximizing the reward, apparently agent is learning to skip the hole and reach the\n",
    "destination."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gymnasium v0.26.2\n",
      "NumPy v1.22.3\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "print(f'Gymnasium v{gym.__version__}')\n",
    "\n",
    "import numpy as np\n",
    "print(f'NumPy v{np.__version__}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize our gym environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', render_mode=\"ansi\", is_slippery=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let us see how the environment looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "print(env.render())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`env.P` below can be used to see the relevant states and rewards for each action taken in that particular state.\n",
    "\n",
    "See: https://github.com/Farama-Foundation/Gymnasium/blob/1fb28ebe7fdafdd9fdf6998536516ef5171109f6/gymnasium/envs/toy_text/frozen_lake.py#L239C68-L239C68\n",
    "\n",
    "Each tuple is a form of (transition probability, new state, reward, terminated).\n",
    "\n",
    "Letting this as `(p, s, r, t)`, return value of `step()` is `(int(s), r, t, False, {\"prob\":p})`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 15, 0, True)],\n",
       " 1: [(1.0, 15, 0, True)],\n",
       " 2: [(1.0, 15, 0, True)],\n",
       " 3: [(1.0, 15, 0, True)]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_state = env.observation_space.n\n",
    "num_action = env.action_space.n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the function called `value_iteration` which performs value iteraion i.e it returns the optimal value\n",
    "function (value table)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma = 1.0):\n",
    "    \n",
    "    # initialize value table with zeros\n",
    "    value_table = np.zeros(num_state)\n",
    "    \n",
    "    # set number of iterations and threshold\n",
    "    no_of_iterations = 100000\n",
    "    threshold = 1e-20\n",
    "    \n",
    "    for i in range(no_of_iterations):\n",
    "        \n",
    "        # On each iteration, copy the value table to the updated_value_table\n",
    "        updated_value_table = np.copy(value_table)\n",
    "        \n",
    "        # Now we calculate Q Value for each actions in the state \n",
    "        # and update the value of a state with maximum Q value\n",
    "        \n",
    "        for state in range(num_state):  # Calculate V(s) for each s\n",
    "            Q_value = np.zeros(num_action)\n",
    "\n",
    "            for action in range(num_action):    # Calculate Q(s,a) for each a | given s.\n",
    "                next_states_rewards = 0\n",
    "                for next_sr in env.P[state][action]:    # Calculate P(R+rV(s')) for each of n possible next states s' | given s & a. (In this environment, n=3 for S or F, n=1 for G or H.)\n",
    "                    trans_prob, next_state, reward, _ = next_sr\n",
    "                    next_states_rewards += trans_prob * (reward + gamma * updated_value_table[next_state])\n",
    "                \n",
    "                Q_value[action] = next_states_rewards   # Q(s,a)|s,a = sum{P(R+rV(s'))}\n",
    "                \n",
    "            value_table[state] = max(Q_value)   # V(s)|s = max_a{Q(s,a)}\n",
    "            \n",
    "        # we will check whether we have reached the convergence i.e whether the difference \n",
    "        # between our value table and updated value table is very small. But how do we know it is very\n",
    "        # small? We set some threshold and then we will see if the difference is less\n",
    "        # than our threshold, if it is less, we break the loop and return the value function as optimal\n",
    "        # value function\n",
    "        \n",
    "        # if (np.sum(np.fabs(updated_value_table - value_table)) <= threshold):\n",
    "        #     print ('Value-iteration converged at iteration #%d.' %(i+1))\n",
    "        #     break\n",
    "    \n",
    "    return value_table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now, we define a function called `extract_policy` for extracting optimal policy from the optimal value function. \n",
    "i.e We calculate Q value using our optimal value function and pick up\n",
    "the actions which has the highest Q value for each state as the optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_policy(value_table, gamma = 1.0):\n",
    " \n",
    "    # initialize the policy with zeros\n",
    "    policy = np.zeros(num_state) \n",
    "    \n",
    "    for state in range(num_state):\n",
    "        \n",
    "        # initialize the Q table for a state\n",
    "        Q_table = np.zeros(num_action)\n",
    "        \n",
    "        # compute Q value for all ations in the state\n",
    "        for action in range(num_action):\n",
    "            for next_sr in env.P[state][action]: \n",
    "                trans_prob, next_state, reward, _ = next_sr \n",
    "                Q_table[action] += (trans_prob * (reward + gamma * value_table[next_state]))\n",
    "        \n",
    "        # select the action which has maximum Q value as an optimal action of the state\n",
    "        policy[state] = np.argmax(Q_table)\n",
    "    \n",
    "    return policy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, We compute the optimal value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0.5905 0.6561 0.729  0.6561]\n",
      "[0.6561 0.     0.81   0.    ]\n",
      "[0.729 0.81  0.9   0.   ]\n",
      "[0.  0.9 1.  0. ]\n"
     ]
    }
   ],
   "source": [
    "optimal_value_function = value_iteration(env=env,gamma=0.9)\n",
    "\n",
    "print()\n",
    "with np.printoptions(precision=4, suppress=True):\n",
    "    for i in range(4):\n",
    "        print(optimal_value_function[4*i : 4*(i+1)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we derive the optimal policy from the optimal value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vv, >>, vv, <<, vv, <<, vv, <<, >>, vv, vv, <<, <<, >>, vv, <<\n"
     ]
    }
   ],
   "source": [
    "optimal_policy = extract_policy(optimal_value_function, gamma=1.0)\n",
    "\n",
    "action = {0:'<<', 1:'vv', 2:'>>', 3:'^^'}\n",
    "arrows = ''\n",
    "for i in range(num_state):\n",
    "    arrows += action[optimal_policy[i]]+', '\n",
    "print(arrows[:-2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note 1. Value iteration for Frozen Lake doesn't seem good. The value of the goal state remains 0 while the values of the other states keep increasing, which makes the last step for success will never be made.\n",
    "- Note 2. If you set `is_slippery=True` for the environment or set `gamma=1.0` at `value_iteration()`, the results will be more bad."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HORLwPython",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
